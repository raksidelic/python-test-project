import os
import logging

# BaÄŸÄ±mlÄ±lÄ±klarÄ± opsiyonel hale getirelim
try:
    from google import genai
except ImportError:
    genai = None

try:
    import openai
except ImportError:
    openai = None

class AIDebugger:
    # VarsayÄ±lanlar
    DEFAULT_PROVIDER = "gemini"
    DEFAULT_GEMINI_MODEL = "gemini-3-flash-preview"
    DEFAULT_OPENAI_MODEL = "gpt-4o"
    
    # Rapor baÅŸlÄ±ÄŸÄ± iÃ§in dinamik deÄŸiÅŸken
    CURRENT_MODEL_NAME = "AI Analysis"

    @staticmethod
    def analyze_error(error_message):
        """
        AI_PROVIDER deÄŸerine gÃ¶re (gemini, openai, all, off) analiz yapar.
        """
        provider = os.getenv("AI_PROVIDER", AIDebugger.DEFAULT_PROVIDER).lower()

        # --- SENARYO 1: AI'YI KAPATMA (OFF) ---
        if provider in ["off", "none", "false", "0"]:
            return None  # HiÃ§bir ÅŸey yapma, None dÃ¶n.

        # ORTAK PROMPTLAR
        system_prompt = (
            "Sen kÄ±demli bir QA Otomasyon MÃ¼hendisisin. "
            "Verilen hata logunu analiz et, kÃ¶k nedeni bul ve Ã§Ã¶zÃ¼m Ã¶ner."
        )
        user_prompt = (
            f"LÃ¼tfen ÅŸu baÅŸlÄ±klarÄ± kullanarak markdown formatÄ±nda yanÄ±tla:\n"
            f"**1. Hata Ã–zeti**\n**2. KÃ¶k Neden**\n**3. Ã‡Ã¶zÃ¼m**\n\n"
            f"--- LOG ---\n{error_message}"
        )

        # --- SENARYO 2: Ä°KÄ°SÄ°NÄ° BÄ°RDEN KULLANMA (ALL) ---
        if provider == "all":
            AIDebugger.CURRENT_MODEL_NAME = "Gemini vs ChatGPT"
            gemini_res = AIDebugger._analyze_with_gemini(user_prompt)
            openai_res = AIDebugger._analyze_with_openai(system_prompt, user_prompt)
            
            # Ä°ki cevabÄ± alt alta birleÅŸtir
            return (
                f"### ğŸ”µ Google Gemini Analizi\n{gemini_res}\n\n"
                f"---\n\n"
                f"### ğŸŸ¢ ChatGPT Analizi\n{openai_res}"
            )

        # --- SENARYO 3: TEKLÄ° SEÃ‡Ä°M ---
        elif provider == "openai":
            return AIDebugger._analyze_with_openai(system_prompt, user_prompt)
        
        elif provider == "gemini":
            return AIDebugger._analyze_with_gemini(user_prompt)
            
        else:
            return f"âš ï¸ Bilinmeyen AI SaÄŸlayÄ±cÄ±sÄ±: {provider}"

    @staticmethod
    def _analyze_with_gemini(prompt):
        if not genai: return "âŒ 'google-genai' kÃ¼tÃ¼phanesi eksik!"
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key: return "âš ï¸ GEMINI_API_KEY eksik!"

        try:
            model = os.getenv("GEMINI_MODEL", AIDebugger.DEFAULT_GEMINI_MODEL)
            if AIDebugger.CURRENT_MODEL_NAME != "Gemini vs ChatGPT":
                AIDebugger.CURRENT_MODEL_NAME = f"Google {model}"
            
            client = genai.Client(api_key=api_key)
            response = client.models.generate_content(model=model, contents=prompt)
            return response.text
        except Exception as e:
            return f"âŒ Gemini HatasÄ±: {str(e)}"

    @staticmethod
    def _analyze_with_openai(system_prompt, user_prompt):
        if not openai: return "âŒ 'openai' kÃ¼tÃ¼phanesi eksik!"
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key: return "âš ï¸ OPENAI_API_KEY eksik!"

        try:
            model = os.getenv("OPENAI_MODEL", AIDebugger.DEFAULT_OPENAI_MODEL)
            if AIDebugger.CURRENT_MODEL_NAME != "Gemini vs ChatGPT":
                AIDebugger.CURRENT_MODEL_NAME = f"OpenAI {model}"
            
            client = openai.OpenAI(api_key=api_key)
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"âŒ OpenAI HatasÄ±: {str(e)}"